# Self-Evaluating-X-NLP
A SELF-EVALUATING, MODEL-AGNOSTIC FRAMEWORK FOR NLP EXPLANATIONS
This repository provides an innovative approach to feature-importance explanations in Explainable Natural Language Processing (X-NLP), aiming to bridge the gap between interpretability and performance of NLP models. Our research presents two novel measures for assessing feature importance. We also develop two post-hoc, model-agnostic methods for quantifying these measures that provides human-grounded explanations and offers detailed insights into model decision-making processes. Our approach is further enhanced by a unique evaluation strategy for function-groundedness that does not rely on benchmark explanations, and an innovative data perturbation technique tailored for masking high-level semantic features. These contributions are evaluated within the biomedicine domain, focusing on span-level NLP tasks such as Semantic Role Labeling. Through comprehensive experiments, we validate the efficacy of our methods in delivering function-grounded explanations across various NLP models, showcasing the adaptability of our explanation methods and their potential applicability beyond NLP. This highlights the importance of our work in advancing the development of transparent, interpretable machine learning models.
